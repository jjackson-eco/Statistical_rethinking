---
title: "Chapters 13 & 14"
subtitle: "Statistical Rethinking: Bayesian inference and data analysis using R and Stan"
author: "John Jackson"
date: "September 2020"
output: 
   rmdformats::material:
      highlight: haddock

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This markdown serves as a companion to Chapters 13 and 14 of the Statistical Rethinking book by Professor Richard McElreath as presented on [YouTube](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA) from his Winter 2019 lectures 15-18, plus additional notes from the book. Chapter13 starts with lecture 15. You need the following packages:

```{r packages, warning=FALSE, message = F}
require(tidyverse)
require(gridExtra)
require(ggridges)
require(rstan)
require(rethinking)
require(dagitty)
```

***

# Lecture 15 - Models with Memory

### An introduction to multi-level Bayesian models

Musicologist called Clive Wearing. In 1985 he got herpes but in his brain. It caused brain damage, affecting much of his hippocampus and he got **anterograde** amnesia. He kept old memories but lost the ability to form new memories. Can still play the piano but can't remember what happened one minute ago. The statistical models we have considered so far, which have all been **fixed effects** models have also had anterograde amnesia. Any time we have moved to a new cluster in our data, the model forgets what it has seen so far.

**Multilevel** models are models that remember. By remember they pool information. Here, the properties of clusters of information in our data come from a population. The inferred population defines pooling. If previous clusters improve your guess about a new clusters, then you want to be using this pooling. However, the order that the clusters are 'visited' does not matter. Imagine you're visiting cafes in various European countries. In a lot of ways they are similar. Berlin and Paris have different types of cafe. How long does the coffee take to come? If we've never been to a cafe, the first time we go we get an expectation. If our coffee took 5 minutes to get to us in Paris, you have an expectation about how long coffee should take to get to you in Berlin, but not exactly the same. The fixed effect only method, we would have no expectation. Also, when we visit either cafe, we also update information **overall** regarding how long coffee takes to arrive generally - so we update our expectation for cafes in both Berlin *and* Paris. We're going to do this statistically.

Consider this more explicitly. We programme a robot to visit two cafes and estimate the waiting time with a Bayesian model. The robot begins with a vague prior for the waiting time with a mean of 5 mins and SD of 1. At the first cafe the robot observes a waiting time of 4 mins, and updates its prior using Bayes theorem, giving it a posterior. However, when it moves to the second cafe, what should its prior be? It could just use the same prior as for the first cafe. However, this assumes implicitly that the cafes have the same average waiting time. So, the robot can do better by treating the cafes as a population, and using the distribution of waiting times across cafes as it's prior expectation. So here, the prior is actually **learned from the data**. So the robot tracks the parameters of waiting times at each cafe, as well as at least two parameters to describe the population of cafes (mean and sd). When the robot observes new cafes, everything is updated, both the estimates for each cafe and the estimates for the population.

How much information is transferred across our units (like cafes) is **dependent on their variation**. If you constantly eat spicy chilli's, you get less stomach bugs. However, they aren't consistent, and the spiciness can vary a lot. Chillis are variable. So if you use your expectation from the whole population, harder to learn and update. Learning this variation is key.

## Cause and Reconciliation

We're fighting a battle on two fronts remember, where there is no unique solution but lots of good ones:

1. Causal inference - don't make causal salad
2. Functional inference - estimation not trivial

So in multilevel model, we're addressing the second one. trying to get more precise estimates. This is assuming our causal model is correct...

As a default we should use multilevel regression. Often little harm in doing this, and more often makes a big difference. Opt-in and opt-out of organ donor lists in Europe. Germany has high percentage of people who think people should donate their organs, but are opt-in, and so their actual organ donation consent percentage (legally) is extremely low. The default is important. Multilevel models are like opt-out systems. Usually estimate better and should have to justify **not** using them. This isn't how things are done though, often assumed that we should start simple and work up.

## The benefits of multilevel models

Dealing with clustering, the Russian dolls. *Pseudoreplication*. Benefits discussed in chapter one repeated here for clarity:

1. **Improved estimates for repeated sampling** - When more than one observation arises from the same individual, location, or time, then single-level models either horribly underfit or overfit the data.
2. **Improved estimates for unbalanced sampling** - When some of these observations are sampled more than others, multilevel models automatically deals with uncertainty across these different levels. This prevents oversampled clusters dominating our inference.
3. **Estimates of variation** - If our research questions include variation among individuals or groups, multilevel models explicitly model variation.
4. **Avoid averaging, retain variation** - Frequently, people average data over clusters to construct variables for analysis in fixed-effects models. Removing variation can be dangerous and our data transformation can be arbitrary. Multilevel models preserve this uncertainty and quantify the variation.

Examples from what we've looked at before - !Kung from families, species in clades, nations in continents, applicants in departments.

Going to learn how shrinkage and pooling work. Now we have to describe the distribution that we think the characteristics of the clusters follow. Maximum entropy helps us here. Also our estimation is trickier, which is aided by our MCMC algorithms with `ulam`. Also a bit trickier to understand because we make predictions at different (multi) levels of our data, making comparisons with WAIC etc. slightly more subtle. We have to make more descisions about the parameters we want to compare for example.

## Example - Multilevel tadpoles

Reed frog tadpoles, quasi-experiment. Eggs suspended on leaves above buckets (ponds). Tadpoles fall in to the water below. Different densities and sizes and predation (presence) in each of the ponds. Interested in the number of surviving tadpoles. Focussing on variation across tanks.

Structure: Tadpoles in tanks at different densities
Outcome: Number surviving (binomial)

We will ignore density and predation for now. We want to know if different tanks have different survival.

Two basic models: 1. Dummy variable for each tank (done this previously) and 2. Multilevel model with varying intercepts.

Our basic model would be like this

$$S_i \sim \operatorname{Binomial}(N_i, p_i)$$
$$\operatorname{logit}(p_i) = \alpha_{\operatorname{TANK}[i]}$$
$$\alpha_j \sim \operatorname{Normal}(0,1.5) \,\,\,\,\,\,\,\,\,\,\, \operatorname{for}j = 1..48$$
Where our survival $S$ for each observation $i$ is given by a binomial distribution with probability $p$, and we estimate a different intercept for each tank $j$ (or pond). Only the data from the tank at hand informs the $\alpha$ from the tank. But surely we expect that survival is linked between.

Let implement this then. First lets look at the data

```{r tadpole data load and clean, fig.width = 11, fig.height = 5}
data(reedfrogs)
tadpole <- reedfrogs %>% 
   mutate(tank = 1:n())

# basic plot
ggplot(tadpole, aes(x = tank, y = surv, fill = surv)) +
   geom_col(show.legend = F) +
   labs(x = "Tank number", y = "Number survived") +
   scale_x_continuous(breaks = 1:48) +
   theme_bw(base_size = 14) +
   theme(panel.grid = element_blank())

# data as a list for our models
tadlist <- list(S = tadpole$surv,
                N = tadpole$density,
                tank = tadpole$tank)
```
We can see here already that there are different numbers of survivals, but very different numbers, probably owing to the density. But ignoring density for now, lets fit our fixed-effect only GLM.

```{r tadpole fixedeff, error = F, warning= F, results = F, mesage = F}

m13.1 <- ulam(alist(S ~ dbinom(N,p),
                    logit(p) <- a[tank],
                    a[tank] ~ dnorm(0,1.5)),
              data = tadlist,
              chains = 4, cores = 4,
              log_lik = TRUE)

```

But now we want to **adaptively pool** information across tanks. We do this with our multilevel model. What we are going to do is allow the intercepts to vary between tanks.

$$S_i \sim \operatorname{Binomial}(N_i, p_i)$$
$$\operatorname{logit}(p_i) = \alpha_{\operatorname{TANK}[i]}$$

$$\alpha_j \sim \operatorname{Normal}(\bar{\alpha},\sigma)$$
$$\bar{\alpha} \sim \operatorname{Normal}(0,1.5)$$
$$\sigma \sim \operatorname{Exponential}(1)$$
We allow the intercepts to vary by adding parameters into our intercepts distribution. This addition of parameters is the multilevel bit. Each $\alpha_j$ has a prior with parameters. And then these parameters have priors themselves, so its just parameters within parameters, priors within priors. This is called a **hyperprior**. Survival across tanks has a distribution, this distribution is the prior for each tank, and this distribution needs its own prior.

## Varying intercepts

Also called random intercepts. Random is silly though. So is varying. It is distinct because you **learn the prior from the data**. We regularise, but now we learn this from the data. The intercepts learn from each other and this pools across.

Lets do it! It's really this easy

```{r tadpole varying intercepts, error = F, warning= F, results = F, mesage = F}

m13.2 <- ulam(alist(S ~ dbinom(N,p),
                    logit(p) <- a[tank],
                    a[tank] ~ dnorm(a_bar,sigma),
                    a_bar ~ dnorm(0,1.5), 
                    sigma ~ dexp(1)),
              data = tadlist,
              chains = 4, cores = 4,
              log_lik = TRUE)

```

We get 50 parameters out of this model with `precis` (don't look at it cos it's big). Two that are $\bar{a}$ and $\sigma$ (the overall parameters for the intercept), and then 48 which are the per-tank intercepts. The fixed effects model would just have 48. Lets compare these models.

```{r tadpole compare}
compare(m13.1, m13.2, func = WAIC)
```
Our predictive performance for our varying intercepts model is better (not hugely - but definite increase). Also, note that for `pWAIC` it is lower for our multilevel model than it is for our fixed effects model. Why? Ended up with a stronger prior. Effective number is also lower than the literal number because we regularised our prior. For multilevel models, **the fit to the sample can decrease but the out of sample fit is better - more regularising**.

## Shrinkage

Now lets compare the intercept differences between our fixed effect only model and our multilevel model. We just have to extract samples from the posterior and average this time. No explicit predictions needed. Just have the posterior survival probabilities for each of the tanks i.e. their intercept.

```{r tadpole post, fig.width = 11, fig.height = 5}

# extract the posterior and raw data
fixeff_post <- extract.samples(m13.1)$a 
reff_post <- extract.samples(m13.2)$a

# posterior survival summaries
tibble(tank = rep(1:nrow(tadpole), times = 2),
       mod = rep(c("fixed", "multilevel"), each = nrow(tadpole)),
       mean = inv_logit(c(colMeans(fixeff_post), colMeans(reff_post))),
       lwr = inv_logit(apply(cbind(fixeff_post,reff_post), 2, PI)[1,]),
       upr = inv_logit(apply(cbind(fixeff_post,reff_post), 2, PI)[2,])) %>% 
   ggplot(aes(x = tank, y = mean, colour = mod, shape = mod)) +
   geom_hline(yintercept = mean(inv_logit(extract.samples(m13.2)$a_bar))) +
   geom_hline(yintercept = mean(tadpole$surv/tadpole$density), colour = "red") +
   geom_errorbar(aes(ymax = upr, ymin = lwr), 
                 width = 0, alpha = 0.5, show.legend = FALSE) +
   geom_point(size = 3) +
   scale_x_continuous(breaks = 1:48) +
   scale_colour_manual(name = "", values = c("cornflowerblue","black")) +
   scale_shape_manual(name= "", values = 16:17) +
   labs(x = "Tank number", y = "Posterior survival probability") +
   theme_bw(base_size = 14) +
   theme(panel.grid = element_blank())

```

Here we have **shrinkage** i.e. we are not just fitting to the sample. Learning the *regular* feature of the sample. Because we have learned the prior from the data adaptively, and pooled information. First, raw mean and posterior mean $\bar{\alpha}$ are different. The raw mean (red) is a lower survival probability. This is because we have imbalanced information across the tanks. The densities vary across tank. So here the $\bar{\alpha}$ is higher, because we have accounted for this density differences.

The main shrinkage phenomenon is that the posterior means shrink towards the population level mean in he multilevel model. Because we have regularised, you see that for low survival tanks the mean shifts up towards the population average (black). As you move further from the mean the more shrinkage there is. This shrinkage decreases though as the density increases i.e. more data with less uncertainty. Shrinkage is the product of a lot of things in our model, but we can easily visualise it. We **reduce our fit to sample to improve our predictive accuracy**. This is similar to regression to the mean phenomenon that we always observe in regression, except here its because of the pooling and thus extreme values regress to the mean.

We can bring this back to poor Ulysses's compass. Varying effects with *partial pooling* (some information shared - not all) are more accurate than fixed effects *without pooling*. The grand mean is maximally underfitting (i.e. if we ignored the tanks and heterogeneity) and also called *complete pooling*. The fixed effects maximally overfit (take each tank independently, but too little data to estimate each mean). These both lead to poor predictions. The varying effects do adaptive regularisation -> better predictions. We are navigating Sylla and Charybdis here adaptively.

If we now focus on $\sigma$ i.e. the variation in our survivals. Minimum of 0 possible with `dexp(1)`, but if it were to be actually set to 0, then there would be no variation between tanks and we converge on the complete pooling scenario. When sigma tends to infinity, we reach the no pooling scenario i.e. the fixed effects model i.e. that all the tanks are infinitely different from each other (completely **independent**). This is intuitive for us, the pooling that is. When we do experiments we consider this variation. The best $\sigma$ is the regularised one that combines information of $\alpha$ across tanks and the information in each tank. We learn the variation from the data itself.

## Our inferred distribution

So what does our population look like? The difference with the previous models is that instead of just having loads of lines in our posterior ($\alpha$s and $\beta$s), we now how a full distribution of our population, captured by $\bar{\alpha}$ and $\sigma$, which have been generated adaptively incorporating the variation in our data. So we need both of these bits of information to build a picture of our population. We need *correlated samples* from our posterior. Drawing our distributions of distributions, correlated pairs of $\bar{\alpha}$ and $\sigma$.

```{r tadpole population, fig.width = 11, fig.height = 6}

post <- extract.samples(m13.2)

tad_abar <- post$a_bar[1:100]
tad_sigma <- post$sigma[1:100]

# Log odds iterating through with ggplot
p <- ggplot() + xlim(-3,4) +
   labs(x = "Log odds of survival", y = "Density") +
   theme_bw(base_size = 13) + theme(panel.grid = element_blank()) 

for(i in 1:100){
   p <- p + stat_function(fun = dnorm, alpha = 0.3,
                          args = list(mean = tad_abar[i], sd = tad_sigma[i]))
}

# Survival probability
sim_surv <- inv_logit(rnorm(8000, post$a_bar, post$sigma)) # remember we made our alpha normal
p_surv <- ggplot(data.frame(sim_surv), aes(x = sim_surv)) +
   geom_density(fill = "blue", alpha = 0.2) +
   scale_x_continuous(breaks = seq(0,1, by = 0.2),limits = c(0,1.111)) +
   labs(x = "Probability of survival", y = "Density") +
   theme_bw(base_size = 14) + theme(panel.grid = element_blank())

grid.arrange(p, p_surv, ncol = 2)

```

## The accuracy advantage

Multilevel models are foundational to navigating the stormy seas and tackle the challenges introduced in Chapter 7. They **are** Ulysses' magic compass. These shrinkage estimates are better. But here we can simulate the accuracy advantage here. We can simulate ponds here under our three assumptions of the future survival between our different ponds: 

1. Complete Pooling - one global intercept to predict all the ponds- Underfit
2. No Pooling - each pond doesn't tell us anything else about the others - Overfit
3. Partial Pooling - Adaptively regularising prior - Multilevel model - Goldilocks

So lets simulate some data from these three scenarios based on the same example experiment from before.

```{r tadpole simulation}

# Model parameters
a_bar <- 1.5
sigma <- 1.5
nponds <- 60
Ni <- as.integer(rep(c(5,10,25,35), each = 15))
p_fullpool <- inv_logit(a_bar) ## the mean probability - assume this is the true value in a full pool

# Simulate pond-level intercepts (log odds of survival for each pond)
set.seed(5005)
a_pond <- rnorm(nponds, mean = a_bar, sd = sigma)

# Create data
tadsim <- data.frame(pond = 1:nponds, Ni = Ni, 
                     true_a = a_pond, 
                     true_p = inv_logit(a_pond),
                     p_fullpool = p_fullpool)

# Simulate survivors from our true intercepts
tadsim$Simsurv <- rbinom(nponds, prob = inv_logit(tadsim$true_a), 
                         size = tadsim$Ni)

# Compute no pooling survival estimates 
tadsim$p_nopool <- tadsim$Simsurv / tadsim$Ni


```

```{r tadsim ulam, error = F, warning= F, results = F, mesage = F}
# Model partial pooling estimates
tadsim_list <- list(Si = tadsim$Simsurv, Ni = tadsim$Ni, pond = tadsim$pond)
m13.3 <- ulam(alist(Si ~ dbinom(Ni,p),
                    logit(p) <- a[pond],
                    a[pond] ~ dnorm(a_bar,sigma),
                    a_bar ~ dnorm(0,1.5), 
                    sigma ~ dexp(1)),
              data = tadsim_list,
              chains = 4, cores = 4,
              log_lik = TRUE)

```

```{r tadsim comparisons, fig.width= 15, fig.height = 6}

# Partial pooling intercepts - pond specific probabilities
tadsim_post <- extract.samples(m13.3)
tadsim$p_partpool <- inv_logit(colMeans(tadsim_post$a))

# Absolute probability error from three scenarios
tadsim_sum <- tadsim %>% 
   mutate(err_fullpool = abs(p_fullpool - true_p),
          err_nopool = abs(p_nopool - true_p),
          err_partpool = abs(p_partpool - true_p)) %>% 
   dplyr::select(pond, Ni, starts_with("err")) %>% 
   pivot_longer(starts_with("err"), names_prefix = "err_", 
                names_to = "Scenario", values_to = "Error")

tadsim_grsum <- tadsim_sum %>% 
   group_by(Ni, Scenario) %>% 
   summarise(mn = mean(Error))

ggplot(tadsim_sum, aes(x = pond, y = Error, colour = Scenario)) +
   geom_hline(data = tadsim_grsum, aes(yintercept = mn, colour = Scenario)) +
   geom_point(size = 4, alpha = 0.6) +
   scale_x_continuous(breaks = 1:60) +
   facet_wrap(~Ni, ncol = 4, scales = "free_x") +
   labs(x = "Pond", y = "Absolute Error") +
   theme_bw(base_size = 14) +
   theme(strip.background = element_blank(),
         panel.grid = element_blank())

```

You can see that particularly at small sample sizes i.e. when there are differences in the density of data in our different groups, the absolute error for the partial pooling method with our regularising random effect is much reduced. Generally, partial pooling reduces the error relative to our true value of the survival probability.

In essence, using multilevel models that have these adaptive, pooling priors is the best way to go.

# Lecture 16 - Multilevel models 2

### Multiple clusters and divergent transitions

We can and in a lot of cases should have more than one type of cluster in our data for our random effects. Natural data is often pooled over both individuals **and** locations **and** time for example. We also need to investigate divergent transitions.

Lets return to prosocial chimpanzees. Left or right pull, with the presence of a partner and and which side or our food is. Answer was no with our binomial model. Here we have multiple clusters - here it is a **cross classification model**. We have actors and experimental block as our clusters. Block effects are a terror in science. Actors have the handedness things, and our blocks are different in time. Not nested here.

So what will our joint model look like here. Because we have hyperpriors for each of our intercept/offset terms (one for actor and one for block), the number of priors goes up a lot. The $\beta$ doesn't have an adaptive prior remember. However, generally it is just the same as before, but with these adaptive priors, and just priors all the way down.

$$
\begin{aligned}
\text{LEFT}_{i} & \sim \operatorname{Binomial}\left(1, p_{i}\right) \\
\operatorname{logit}\left(p_{i}\right) &=\alpha_{\text {ACTOR }[i]}+\gamma_{\text {BLOCK }[i]}+\beta_{\text {TREATMENT }[i]} \\
\beta_{j} & \sim \text { Normal }(0,0.5), \text { for } j=1 . .4 \\
\alpha_{j} & \sim \operatorname{Normal}\left(\bar{\alpha}, \sigma_{\alpha}\right), \text { for } j=1 . .7 \\
\gamma_{j} & \sim \operatorname{Normal}\left(0, \sigma_{\gamma}\right), \text { for } j=1 . .6 \\
\bar{\alpha} & \sim \operatorname{Normal}(0,1.5) \\
\sigma_{\alpha} & \sim \text { Exponential }(1) \\
\sigma_{\gamma} & \sim \text { Exponential }(1)
\end{aligned}
$$
Our $j$s are the sublevels of each of our variables. The $\alpha$s are the varying intercepts for our actors (Chimpanzee), which is adaptive i.e. a random effect. We also have our varying intercept for our Block, which is also a random effect. However, we don't have a $\bar{\gamma}$ here, and instead set the mean to 0. Adding this would be redundant, because then you're using two terms to learn the intercept overall. This is still adaptive though, because we're controlling the variance. Lets load.

```{r chimp load}
data("chimpanzees")
chimpanzees$treatment <- 1 + chimpanzees$prosoc_left + 2*chimpanzees$condition
chimp_list <- list(pulled_left = chimpanzees$pulled_left,
                   actor = chimpanzees$actor,
                   block_id = chimpanzees$block,
                   treatment = as.integer(chimpanzees$treatment))
```

And lets build this model in `ulam`, good to put notes in saying what each of the sections of the model are.

```{r chimp multiple clusters,  error = F, results = F, mesage = F}

set.seed(13)
m13.4 <- ulam(alist(
               # The model 
               pulled_left ~ dbinom(1, p),
               logit(p) <- a[actor] + g[block_id] + b[treatment],
   
               # Simple prior
               b[treatment] ~ dnorm(0,0.5),
               
               # Adaptive priors - our RE marker
               a[actor] ~ dnorm(a_bar, sigma_a),
               g[block_id] ~ dnorm(0, sigma_g),
               
               # Hyperpriors
               a_bar ~ dnorm(0,1.5),
               sigma_a ~ dexp(1),
               sigma_g ~ dexp(1)),
       data = chimp_list, chains = 4, cores = 4, log_lik = TRUE)

```

Notice that we have a warning about **Divergent Transitions** - more on this in a moment. Let's have a look at the output from this. This time we will look at the posterior distribution summary, and then the standard deviation terms for our two adaptive priors, $\sigma_{\alpha}$ and $\sigma_{\gamma}$ i.e. our random effect variances.

```{r chimp precis, fig.height = 6.5, fig.width= 4}
plot(precis(m13.4, depth = 2))
```

```{r chimp post, fig.height = 3, fig.width= 6}
cpost <- extract.samples(m13.4)

bind_rows(tibble(term = "Block", post = cpost$sigma_g),
          tibble(term = "Actor", post = cpost$sigma_a)) %>% 
   ggplot(aes(x = post, fill = term)) +
   geom_density(alpha = 0.8) +
   scale_fill_manual(values = c("firebrick", "cornflowerblue"), 
                     name = "Varying\neffect") +
   labs(x = "Standard deviation", y = "Density") +
   theme_bw(base_size = 14) +
   theme(panel.grid = element_blank())

```

We can see here that the $\gamma$ or g block effects are all very close to zero, and that the variance for our block effect is effectively 0, indicating that there isn't much variance across blocks. Actor 2's posterior is very wide, but all of these values are bunched up right at the boundary i.e. at 1, a[2] was a leftie.

If we then compare this to a random effects model without or block effects, the number of parameters drops by 7, but the effective number of parameters doesn't drop as much as you would think. So it would seem that incorporating block doesn't have benefits, but actually didn't incur much of a cost either. The inference of the other effects remain similar in this case. Because of shrinkage. By aggressively regularising, we can have lots of parameters and still get similar inferences.

## Even more clusters

A bad definition of random effects is that random effects are not fixed by the experimenter. However, the source of the variance doesn't matter, we want the pooling for estimating benefits. If you care about better inference, use adaptive priors. Can use adaptive priors for the $\beta$ terms too, works pretty well too. The estimates don't change very much, we have lots of data for each treatment though.

## Divergent Transitions

These are your friend. Tell you that something is numerically inefficient in the Markov Chain. Can fix them. You can sometimes fix this with changing a parameter, but sometimes need to reparameterize.

Imagine you're on a frictionless rollercoaster. Two forms of energy that have to be conserved. If the rollercoaster moves down it looses potential energy and gains kenetic, and then when it goes up it is the other way around. Our Markov chain is like this. A divergent transition is when the rollercoaster cart comes of the track. The Hamiltonian method calculates the energy in the system. We have transitions which are our sample paths, the movement of the particle around the rollercoaster of the posterior. If the energy at the end of a transition is not the same as that at the start, then it is **divergent**. Indicates an inaccurate approximation. Tends to happen in regions of strong curvature, where we pop off the track. **Happens a lot if parameters are conditional on other parameters too - Like in adaptive priors**. Hamiltonian is good because it tells us about this - other algorithms don't.

We can display this with the **Devils Funnel**, a two parameter posterior with a model like this

$$v \sim \text{Normal}(0,3)$$
$$z \sim \text{Normal}(0, \exp v)$$
Where for our two normal priors, the $z$ depends on the $v$. This creates a really steep curvature in the 2D parameter space for our posterior, and thus our transitions can become divergent, i.e. the rollercoaster can jump off the track. No single step size can help with this - need small step sizes when it is steep and bigger ones when it is shallow. Doesn't corrupt your chain but can mean it is less efficient, and can mean your ineffective samples make a bad guess of the posterior.

Two ways to deal with it:

1. Increase adapt-delta.
2. Re-parametrise.

But first check if you have left a prior out or something like that. Surprisingly common.

## Non-centered parameterisation

We can reparameterise by expressing our statistical model in another way, that is identical. Take this statistical model where we are investigating $\alpha$, which is given by the following distribution

$$\alpha \sim \text{Normal}(\mu,\sigma)$$
This is the centered parametrisation though, for which the distribution is dependent on more than one parameter. But we can rearrange this (identically) in a re-centered way

$$\alpha = \mu + \beta$$
$$\beta \sim \text{Normal}(0,\sigma)$$
They are the same, just have taken $\mu$ out. Why do this though? Could go even further, and write this in a fully **non-centered** way, taking all parameter dependencies out.

$$\alpha = \mu + z\sigma$$
$$z\sim \text{Normal}(0,1)$$
Although these are the same mathematically, our HMC algorithm sees a different geometry, a different shape, which helps a lot with our sampling. All this because we don't sample our parameters directly. Think of this like **standardisation** of our variables in the model. Why is it called centered? Just means that a distribution is conditional on one or more parameters. Fully **non-centered** means that we take all the conditioning out and put it in a linear model. In short we get many more effective samples from our Markov Chain if we do a non-centered parameterisation. 

## Reparameterising our Chimp model

So, with this information in mind we can repeat this for our chimp model, and do non-centered parameterisation so our adaptive priors are not dependent on other parameters. We do this by rewritting our linear model so that we have z-scores and sigmas inside of them. Each actor effect for example is some z-score for each chimp, multiplied by the variance of each chimp, and then our global intercept. This does not not look as intuitive, but is very useful. Our model will now look like this

$$
\begin{aligned}
\text{LEFT}_{i} & \sim \operatorname{Binomial}\left(1, p_{i}\right) \\
\operatorname{logit}(p_{i}) &= \bar{\alpha} + z_{\text {ACTOR }[i]}\sigma_{\alpha} + x_{\text {BLOCK }[i]}\sigma_{\gamma} +\beta_{\text {TREATMENT }[i]} \\
\beta_{j} & \sim \text {Normal}(0,0.5), \text { for } j=1 . .4 \\
z_{j} & \sim \operatorname{Normal}(0,1), \text { for } j=1 . .7 \\
x_{j} & \sim \operatorname{Normal}(0,1), \text { for } j=1 . .6 \\
\bar{\alpha} & \sim \operatorname{Normal}(0,1.5) \\
\sigma_{\alpha} & \sim \text {Exponential}(1) \\
\sigma_{\gamma} & \sim \text {Exponential}(1)
\end{aligned}
$$
The varying effects are now standardised (see the 0,1 normals for our two z scores - x is just chosen for notation but are both z scores). This is a very important feature of varying effects models that we need to consider.

And now for our Stan model. This will have posteriors that are the same, but there are some changes to our model code.

```{r chimp non-centered,  error = F, results = F, mesage = F}
set.seed(13)
m13.4nc <- ulam(alist(
               # The model 
               pulled_left ~ dbinom(1, p),
               logit(p) <- a_bar + z[actor]*sigma_a + z[block_id]*sigma_g + b[treatment],
   
               # Simple prior
               b[treatment] ~ dnorm(0,0.5),
               
               # Adaptive priors - our RE marker
               z[actor] ~ dnorm(0, 1),
               x[block_id] ~ dnorm(0, 1),
               
               # Hyperpriors
               a_bar ~ dnorm(0,1.5),
               sigma_a ~ dexp(1),
               sigma_g ~ dexp(1),
               gq> vector[actor]:a <<- a_bar + z*sigma_a,
               gq> vector[block_id]:g <<- x*sigma_g),
       data = chimp_list, chains = 4, cores = 4, log_lik = TRUE)

```

The linear model has changed such that our parameters go in there, but this is just a direct translation of the model equation above. Notice in particular our additional `gp>` parts of our model code, which is telling stan to do some additional calculations for us. Namely, it is saying that we want to combine our output from $\bar{\alpha}$, $z$, and $\sigma_a$ to form our single parameter for our actor effect. The same goes for our block id effect.

```{r, chimp nonc summary}
precis(m13.4nc, depth = 2)
```

The benefits of this re-parameterisation:

1. The chain runs faster.
2. Large increase to effective sample number.
3. No divergent transitions.

## Posterior predictions

Now we want to do some predictions using our multilevel models. The first thing we have to note is that because we have done partial pooling, and have shrinkage towards a population level mean, our predictions may not fit our sample as well as before. This will particularly be the case for clusters with a small sample size with a low density of data. 

For actually doing the predictions themselves, the thing that the addition of random effects does is that it makes your predictions a little bit more subtle. You have to make some choices i.e. deciding what level of the model you are interested in reporting.

> Are you interested in new clusters, or the same clusters?

So, we have learned about some parameters both at the population level and at the cluster level. If you're interested in the same clusters that you have used, works very much like before i.e. with `sim` and `link`. Treat your varying effects as parameters and just push samples back through the model. This is if you want to predict for your sample essentially. Here you take into account your adaptive priors. For the chimpanzees, think of this where we predict for the `chimpanzees` data itself.

However, we may also want to make predictions for new clusters, for which our additional information from the adaptive priors may not be as useful and instead we're just interested in the population-level mean. For the chimps, we would want to sample from new actors. And now Richard shows us three we have to decide whether we want to use:

1. The average cluster - population mean
2. The marginal of cluster - sample a bunch of individuals in our cluster and average over them incl. variation
3. Sample of actors from posterior - Show your posterior of different individuals in the cluster as new lines.

## Average actor

Here we want to take the effect at the population mean or at $\bar{\alpha}$ in our chimpanzee example. Here, we just want to do a prediction that incorporates our population-level intercept and our treatment $\beta$ effects. So this is useful for looking at population-level treatment effects, things that are captured in our $\beta$ terms. I.e. this is often the average one you'll want for papers.

```{r av actor chimp, fig.width= 7, fig.height = 5}
# posterior samples
post_chimp <- extract.samples(m13.4nc)

# function to combine specific parts of the posterior (only treatment and a_bar here)
p_link_abar <- function(treatment){
   logodds = with(post_chimp, a_bar + b[,treatment])
   return(inv_logit(logodds))
}

# predict for our four treatments
pdat <- tibble(lab = c("Right prosocial\nNo partner", "Left prosocial\nNo partner", 
               "Right prosocial\nPartner", "Left prosocial\nPartner"),
       treatment = 1:4, mu = NA, lwr = NA, upr = NA) 

for(i in 1:nrow(pdat)){
   preds = p_link_abar(pdat$treatment[i])
   pdat$mu[i] = mean(preds)
   pdat$lwr[i] = PI(preds)[1]
   pdat$upr[i] = PI(preds)[2]
}

# plot
ggplot(pdat, aes(x = factor(lab, levels = lab), y = mu)) +
   geom_errorbar(aes(ymax = upr, ymin = lwr), width = 0.03) +
   geom_point(size = 3) +
   lims(y= c(0,1)) +
   labs(x = "", y = "Posterior proportion pulled left") +
   theme_bw(base_size = 14)

```

## Marginal of actor - sample actors

Now we want to sample a bunch of different chimpanzees and average over them, to incorporate the variation in the cluster as well as just the mean. We can then either just plot these different simulated actors, or summarise over them to get our marginalised effect, which includes more variation.

```{r marg actor chimp, fig.width= 12, fig.height = 5}
# Simulate different intercepts i.e. individuals in our cluster from the model parameters a_bar and sigma_a
a_sim <- with(post_chimp, rnorm(length(post_chimp$a_bar), a_bar, sigma_a))

# function to combine specific parts of the posterior (a_sim and treatment)
p_link_asim <- function(treatment){
   logodds = with(post_chimp, a_sim + b[,treatment])
   return(inv_logit(logodds))
}


# 1. Marginal summary
# predict for our four treatments
pdat <- tibble(lab = c("Right prosocial\nNo partner", "Left prosocial\nNo partner", 
               "Right prosocial\nPartner", "Left prosocial\nPartner"),
       treatment = 1:4, mu = NA, lwr = NA, upr = NA) 

for(i in 1:nrow(pdat)){
   preds = p_link_asim(pdat$treatment[i])
   pdat$mu[i] = mean(preds)
   pdat$lwr[i] = PI(preds)[1]
   pdat$upr[i] = PI(preds)[2]
}

# plot
marg1 <- ggplot(pdat, aes(x = factor(lab, levels = lab), y = mu)) +
   geom_errorbar(aes(ymax = upr, ymin = lwr), width = 0.03) +
   geom_point(size = 3) +
   lims(y= c(0,1)) +
   labs(x = "", y = "Posterior proportion pulled left") +
   theme_bw(base_size = 14)

# Marginal actors
marg2 <- data.frame(sim = 1:length(post_chimp$a_bar),
              sapply(1:4, function(i) p_link_asim(i))) %>% 
   pivot_longer(-sim, names_prefix = "X") %>% 
   mutate(name = as.integer(name),
           lab = rep(c("Right prosocial\nNo partner", "Left prosocial\nNo partner", 
               "Right prosocial\nPartner", "Left prosocial\nPartner"),
               times = length(post_chimp$a_bar))) %>% 
   filter(sim <= 200) %>% 
   ggplot(aes(x = lab, y = value, group = sim)) +
   geom_line(alpha = 0.2, size = 1.3) +
   lims(y= c(0,1)) +
   labs(x = "", y = "Posterior proportion pulled left") +
   theme_bw(base_size = 14)

grid.arrange(marg1, marg2, ncol = 2)

```

## Post-stratification

One final thing to mention is if we have non-representative samples of differing densities. We may want to account for this in our predictions even more. To do this, we can do **post-stratification**, where by we weight our estimates for each of our differing individuals (groups in our cluster) based on their sample size. This doesn't always work, particularly if there is causal links between non-representative samples and our outcome.

# Lecture 17 - Adventures in Covariance

### Extending multilevel models

So far we have just been varying the intercepts i.e. the averages are differing by cluster. Pooling improves our estimates. Even though we bias our estimate, it is less biased actually i.e. better predictions out of sample. Any parameter in our model can be turned into a varying effect.

However, we can extend this strategy to out **slopes** i.e. that the effect of our predictor varies by cluster. Again, you know this already but there are lots of good reasons why we might want to allow the slope to vary in a multilevel model. Do drug levels affect different people differently, after-school programmes don't work for everyone, different life-history trajectories for different individuals. *Average* effect misleading? Can be depending on the clusters - not every unit has the same response.

Could treat them the same as the intercepts with adaptive pooling priors. Just shove another adaptive prior in our linear model equation. However we can do even better though by relating our intercepts to our slope. They have **covariance** with each other. Learning one gives you some info about the other, as you change the slope of the line the intercept changes (e.g. higher intercepts have smaller slopes). We want to have a single prior, which brings together **features** of the different units in our population that has correlation between these features i.e. the intercepts and the slopes. So we want to do correlated population pooling. This correlation structure lets us transfer information across these features.

## Cafe Robot

Lets demonstrate this with our cafe robot example again. Coffee waiting times between different cities again. Pool the waiting times across our clusters of different cities. However, now we want to know whether the coffee waiting times are different depending on the time of day as well. Our intercept is our average waiting time in the morning, and our slope is the change in waiting time from morning to afternoon. Will our slopes and intercepts be related? Yes. There is a negative correlation between average weight time in the morning (intercept) and our difference in morning and afternoon (slope). If you have higher morning waiting time, your slope (diff) is smaller because there isn't a saturation effect. This correlation gives us information, which we will do well by using. 

Now we are 













